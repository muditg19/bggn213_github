---
title: "Class 8: PCA mini Project"
author: "Mudit"
format: pdf
---

```{r}
head(mtcars)
```

Let's look at the mean of every row and column:

```{r}
apply(mtcars, 2, mean)
```
Let's look at "spread" via `sd()`

```{r}
apply(mtcars, 2, sd)
```

```{r}
pca <- prcomp(mtcars)
biplot(pca)
```

Let's try scalling the data:

```{r}
mtscale <- scale(mtcars)
head(mtscale)
```
what is the mean of each dimmension/column in mtscale?
```{r}
round(apply(mtscale, 2, mean), 3)
```

what is the standard deviation of each dimmension/column in mtscale?
```{r}
apply(mtscale, 2, sd)
```

Let's plot `mpg` vs `disp` for both mtcars and after the scalling


```{r}
library(ggplot2)
ggplot(mtcars) + 
  aes(mpg, disp) +
  geom_point()
```

```{r}
library(ggplot2)
ggplot(mtscale) + 
  aes(mpg, disp) +
  geom_point()
```

```{r}
pca2 <- prcomp(mtscale)
biplot(pca2)
```

## Breast Cancer FNA data

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
```

```{r}
wisc.df
```

> q.1 How many rows/patients/subjects.

```{r}
nrow(wisc.df)
```

> Q.2 How many M (cancer) B(healthy) patients

```{r}
table(wisc.df$diagnosis)
```

```{r}
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
```

```{r}
# Create diagnosis vector for later 
diagnosis <- as.factor(wisc.df$diagnosis)
```

> Q.3 How many variables/features in the data are suffixed with _mean?

```{r}
length(grep("_mean", colnames(wisc.data), value = TRUE))
```

```{r}
# Check column means and standard deviations
colMeans(wisc.data)
```

```{r}
apply(wisc.data,2,sd)
```



```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale = TRUE)
```

How much variance is captured in each PC?
```{r}
# Look at summary of results
x <- summary(wisc.pr)
x
```
>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
44.27% 
>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
3 (PC1, PC2, PC3)

```{r}
sum(x$importance[2, 1:3])  
```


>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
7

```{r}
sum(x$importance[2, 1:7])  
```

```{r}
screeplot(wisc.pr, type = "lines")
```
```{r}
plot(x$importance[2,], typ="b")
```
```{r}
attributes(wisc.pr)
```
```{r}
head(wisc.pr$x)
```
> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

```{r, warning=FALSE}
biplot(wisc.pr)
```
This plot is not providing a lot of information with the mess it has. I will need more a plot with which I can get more sense of the PCA data

My main PC result figure
```{r}
# Scatter plot observations by components 1 and 2
plot( wisc.pr$x[, 1], wisc.pr$x[, 2] , col = diagnosis , 
     xlab = "PC1", ylab = "PC2")
```
>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?


```{r}
# Repeat for components 1 and 3
plot( wisc.pr$x[, 1], wisc.pr$x[, 3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC.

```{r}
wisc.pr$rotation["concave.points_mean",1]
```

## 3. Hierarchical clustering

```{r}
data.scaled <- scale(wisc.data)
```

```{r}
data.dist <- dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```

# Results of hierarchical clustering

Let’s use the hierarchical clustering model you just created to determine a height (or distance between clusters) where a certain number of clusters exists.

>Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
#clusters <- cutree(wisc.hclust, k = 4)
#table(clusters)
abline(h = 19, col="red", lty=2)
```
>Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

ward.D2 - minimises the variance within clusters. We get compact and even clusters


#Selecting number of clusters
In this section, you will compare the outputs from your hierarchical clustering model to the actual diagnoses. Normally when performing unsupervised learning like this, a target variable (i.e. known answer or labels) isn’t available. We do have it with this dataset, however, so it can be used to check the performance of the clustering model.

When performing supervised learning - that is, when you’re trying to predict some target variable of interest and that target variable is available in the original data - using clustering to create new features may or may not improve the performance of the final model.

This exercise will help you determine if, in this case, hierarchical clustering provides a promising new feature.

Use cutree() to cut the tree so that it has 4 clusters. Assign the output to the variable wisc.hclust.clusters.

```{r}
# Extract the first 7 principal components
pc_data <- wisc.pr$x[, 1:7]

# Compute the distance matrix
pc_dist <- dist(pc_data)

# Perform hierarchical clustering using Ward's D2 method
wisc.pr.hclust <- hclust(pc_dist, method = "ward.D2")

wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```
> Q13. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
table(wisc.pr.hclust.clusters , diagnosis)
```

> Q14. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
#We can use the table() function to compare the cluster membership to the actual diagnoses.
table(wisc.hclust.clusters, diagnosis)
```

Here we picked four clusters and see that cluster 1 largely corresponds to malignant cells (with diagnosis values of 1) whilst cluster 3 largely corresponds to benign cells (with diagnosis values of 0).

Before moving on, explore how different numbers of clusters affect the ability of the hierarchical clustering to separate the different diagnoses.


```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)

# View the first few cluster assignments
table(wisc.hclust.clusters, diagnosis)
```

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:3]), method="ward.D2")
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)

```
```{r}
wisc.pr$x[1:3]
```


```{r}
d <- dist(wisc.pr$x[,1:3])
 hc <- hclust(d, method = "ward.D2")
plot(hc)
```
Compare to my expert A and B in `diagnosis`
```{r}
table(diagnosis)
```

```{r}
table(diagnosis, grps)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

> Q16. Which of these new patients should we prioritize for follow up based on your results?


```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

Patient 2 should be priortized based on these results as they are closer to the cluster of malignant patients data points

```{r}
loadings <- wisc.pr$rotation

ggplot(loadings) +
  aes(abs(PC1), reorder(rownames(loadings), -PC1)) + 
  geom_col()
```

